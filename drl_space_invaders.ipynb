{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Note: If the installation failes, try to remove the single quotes from `gym[atari]`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "!pip install tensorflow==2.3.1 gym keras-rl2 'gym[atari]'"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## 1. Test Random Environment with OpenAI Gym"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "source": [
    "### `import gym`\n",
    "\n",
    "Imports the OpenAI gym library.\n",
    "\n",
    "### `import random`\n",
    "\n",
    "Imports the random library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "source": [
    "### `env = gym.make('SpaceInvaders-v0')`\n",
    "\n",
    "Create the Space-Invaders gym environment.\n",
    "This is the frame based version (there is also a RAM-based version available).\n",
    "\n",
    "\n",
    "### `env.observation_space.shape`\n",
    "\n",
    "Retrieving a first image from the environment, which is part of the v0 state.\n",
    "Extracting the shape of the image.\n",
    "This is needed for the neural network later on.\n",
    "\n",
    "`height, width` -> height and width of the image.\n",
    "\n",
    "`channels` -> color channels of the image.\n",
    "\n",
    "### `actions = env.action_space.n`\n",
    "\n",
    "This provides the number of available actions. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "source": [
    "### `env.unwrapped.get_action_meanings()`\n",
    "\n",
    "Get the action names which are available.\n",
    "This are the actions our agent can actually take inside of the environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 0 -- Score: 230.0\n",
      "Episode: 1 -- Score: 275.0\n",
      "Episode: 2 -- Score: 50.0\n",
      "Episode: 3 -- Score: 60.0\n",
      "Episode: 4 -- Score: 135.0\n"
     ]
    }
   ],
   "source": [
    "episodes:int = 5 # playing 5 episodes of Space invaders\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done: bool = False\n",
    "    score: int = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([n for n in range(6)])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        # print('reward: {} -- done: {} -- info: {}'.format(reward, done, info))\n",
    "        score += reward\n",
    "    print('Episode: {} -- Score: {}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "source": [
    "### `state = env.reset()`\n",
    "\n",
    "Resets the environment and returns the initial state.\n",
    "\n",
    "### `done: bool = False`\n",
    "\n",
    "If this variable is set to `True`, the game has finished and we can stop.\n",
    "Initially this variable is set to `False`.\n",
    "\n",
    "### `score: int = 0`\n",
    "\n",
    "Setting the game score to 0.\n",
    "This will be used as a measuremeht of the random actions taken.\n",
    "\n",
    "### `action = random.choice([n for n in range(6)])`\n",
    "\n",
    "Takes a random action in the range of 0 to 5.\n",
    "\n",
    "The available actions are:\n",
    "\n",
    "`['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']`.\n",
    "\n",
    "Therefore the agents simply takes random actions in the environment and has no strategy for playing the game and will not learn one with this approach.\n",
    "\n",
    "This allows us to see how a random set of choices perform inside of Space Invaders.\n",
    "\n",
    "\n",
    "### `n_state, reward, done, info = env.step(action)`\n",
    "\n",
    "`env.step(action)` applies the (randomly) chosen action and applies it on the agent.\n",
    "As a result, the current state of the environment gets updated and this update is returned by this function.\n",
    "Extract the next state, the reward for taking that action, if the game is done and some info about the game.\n",
    "\n",
    "\n",
    "### `score += reward`\n",
    "\n",
    "Each step taken increments our score with the given reward based on the current action.\n",
    "\n",
    "### `print('Episode: {} -- Score: {}'.format(episode, score))`\n",
    "\n",
    "Once the game of the current episode is done, the score of it gets printed.\n",
    "\n",
    "### `env.close()`\n",
    "\n",
    "Closes the environment.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Create a Deep Learning Model with Keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "### `from tensorflow.keras.models import Sequential`\n",
    "\n",
    "Imports the `Sequential` API.\n",
    "\n",
    "This allows us to build sequential deep learning models.\n",
    "\n",
    "\n",
    "### `from tensorflow.keras.layers import Dense, Flatten, Convolution2D`\n",
    "\n",
    "Imports different types of layers needed for the deep learning model.\n",
    "\n",
    "\n",
    "#### `Convolution2D`\n",
    "Because we only receive an image from our Space-Invaders environment, the `Convolution2D` is needed for this type of input.\n",
    "\n",
    "This scans through the image and analyses how the model is actually performing.\n",
    "\n",
    "### from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "The `Adam` algorithm is used as an optimizer once the deep learning model is compiled.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):",
    "    model: Sequential = Sequential()\n",
    "    model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(3, height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())"
   ]
  },
  {
   "source": [
    "### `def build_model(height, width, channels, actions)`\n",
    "\n",
    "The parameters in this function will define how the DL model will look like.\n",
    "\n",
    "Because the model is image based, the model will start with convolution layers. \n",
    "\n",
    "#### `model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(3, height, width, channels)))`\n",
    "\n",
    "The `add` function is used to start stacking layers inside the deep neural network.\n",
    "\n",
    "\n",
    "Now a short explanation of the parameters of `Convolution2D`:\n",
    "- `32` -> is the number of filters. So the model has 32 convolutional 2d filters. These filters are trained to detect different things within the environments images.     Best case would be filters which detects enemies, the mothership, where the protecting bases/houses are, and so on.\n",
    "- `(8, 8) -> specify how big this filters are going to be. So in this case, 8 units by 8 units.\n",
    "- `strides=(4, 4)` -> The stride defines how the filter window passes through the image. In this case, 4 strides to the right, and 4 strides down, so the filter            window is moving diagonally.\n",
    "- `activation='relu'` -> the activation function, in this case a `relu` function.\n",
    "- `input_shape=(3, height, width, channels)` -> the 3 denotes that the model will pass through multiple frames from our reinforcement learning model to the deep learning model.\n",
    "\n",
    "*After this, multiple Convolution2D layers will be added to the model*.\n",
    "\n",
    "\n",
    "#### `model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))`\n",
    "\n",
    "Uses more filters but the filters are going to be smaller than in the first layer.\n",
    "\n",
    "The stride distance is also smaller than the one of the first layer and the activation function is yet again the `relu` activation function.\n",
    "\n",
    "\n",
    "#### `model.add(Convolution2D(64, (3, 3), activation='relu'))`\n",
    "\n",
    "Uses 3 by 3 filters and since the stride size is not defined for this Convolution2D layer, the stride distance defaults to 1 by 1.\n",
    "So it is going pixel by pixel.\n",
    "\n",
    "\n",
    "#### `model.add(Flatten())`\n",
    "\n",
    "This step takes all convolutional layers and flattens them into a single layer.\n",
    "\n",
    "This is done so the model can be passed to a dense layer.\n",
    "\n",
    "\n",
    "### Dense layers\n",
    "\n",
    "Dense layers are also known as **fully connected layers**.\n",
    "So every element in a layer is connected with every element of the next layer.\n",
    "As seen above, in order to use the current model structure, the convolutional layers have to be flattened into a single layer.\n",
    "\n",
    "17:10\n",
    "\n",
    "https://www.youtube.com/watch?v=hCeJeq8U0lo\n",
    "\n",
    "https://www.youtube.com/watch?v=cO5g5qLrLSo\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Build Agent with Keras-RL\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Reloading Agent from Memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
